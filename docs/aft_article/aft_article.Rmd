---
title: A bag-of-tales from Santa
subtitle: Converting the Ashliman Folktexts Collection into a dataset for machine learning
titlerunning: Bag-of-tales
authorrunning: Darányi, S. & Hagedorn, J.
thanks: | 
    Grants or other notes about the article that should go on the front 
    page should be placed here. General acknowledgments should be placed at the
    end of the article.

authors: 
- name: Sándor Darányi
  address: Swedish School of Library and Information Science, University of Borås
  email: sandor.daranyi.hb.se@gmail.com
  
- name: Joshua Hagedorn
  address: Department of ZZZ, University of WWW
  email: josh.hagedorn@gmail.com

keywords:
- key
- dictionary
- word

#PACS: 
#- PAC1
#- superPAC
    
# MSC:
# - MSC code 1
# - MSC code 2

abstract: |
  Computational motif identification in folktales is an open research problem. To move ahead in this area, the field would benefit from shared test data for machine learning, putting experimentation in focus. Folklore databases including text collections in multiple languages do exist, but not in dataset form for data science, and are currently not shared, making their results non-reproducible, an obstacle to scientific progress. The need for significant preprocessing adds insult to injury, rendering the outcome both incomparable and subject to multidisciplinary criticism. As a first step to remedy this problem, we report work in progress, having converted the Ashliman Folktexts Collection into a public dataset for supervised tale type learning, itself a precondition for scalable motif identification. In the future, this dataset can be upgraded in several respects to serve as the basis for springboard experiments with the Thompson Motif Index and the Aarne-Thompson-Uther tale typology, paving the way for ontology development.

bibliography: bibliography.bib
biblio-style: spbasic
# bibstyle options spbasic(default), spphys, spmpsci
output: rticles::springer_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = F,message = F,error = F,warning = F,
  fig.width=6,fig.asp = 0.618,dpi=300,dev = "cairo_pdf"
)
library(tidyverse); library(httr); library(rvest); library(tidytext); library(fuzzyjoin)
```

```{r themes, echo=FALSE, message=FALSE}
extrafont::loadfonts(device="win")
font_family = "Gill Sans MT"

theme_bar <- 
  theme_minimal() + 
  theme(
    plot.title.position = "plot",
    text = element_text(family = font_family),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(size=.1, color="grey" ),
    panel.grid.minor.y = element_line(size=.1, color="grey" ),
    axis.text.x.bottom = element_text()
  )

theme_bar_flip <- 
  theme_minimal() + 
  theme(
    plot.title.position = "plot",
    text = element_text(family = font_family),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_line(size=.1, color="grey" ),
    panel.grid.minor.x = element_line(size=.1, color="grey" ),
    axis.text.x.bottom = element_text()
  )
```

# Introduction {#intro}

Ever since the concept of a motif was introduced some 200 years ago, the quest to identify content elements above word level has been a standard preoccupation in literary science \cite{alma991024476189703276}, \cite{nla.cat-vn1542191}. There a motif stands for a recurrent theme, whereas in musicology, a motive is considered "the smallest structural unit possessing thematic identity" \cite{white1976analysis}. In a similar vein, Stith Thompson defined motifs in folktale research as "the smallest element in a tale having a power to persist in tradition" \cite{ nla.cat-vn2560920} (1946).

A sufficient overlap between these definitions suggests that such higher order content units exist as narrative building blocks in a generic sense, but their automatic extraction by computational means has eluded folk narrative studies so far \cite{daranyi2010proceedings}. In spite of the suggestion that topics identified by Labeled Latent Dirichlet Allocation (L-LDA) had an analogous function with motifs in a database of Dutch and Frisian folktales \cite{2d77156b39724b4b86eeca60d508f678}, we consider finding characteristic patterns of semantic content an open research problem. One reason for our skepticism is that in Thompson’s Motif Index of Folk Literature \cite{ nla.cat-vn857541} alone, over 45000 motifs are listed on a global scale, but many more e.g. regional motif indexes exist whose material would doubtlessly inflate that number. As we will argue below, digital humanities (DH) in general, and folk narrative studies in particular, are not up to the task of a scalable pattern hunt yet.

<!-- Add numbers from TMI/ATU mapping -->

Our research problem for the current paper is this: consider the case of two standard reference tools, the TMI, and the Aarne-Thompson-Uther tale typology \cite{uther2004types}. A count in the TMI indicated the presence of …. motifs, whereas Yarlott and Finlayson counted 46,248 motifs and sub-motifs from over 614 collections, 41,796 of which had references to tales or tale types \cite{yarlott_et_al:OASIcs:2016:6708}. However, based on our count the ATU uses only …. (…%) of them to model tale structures as motif strings. One is then prompted to ask, where have …. % of motifs in the TMI disappeared, and how can an important monograph acquire almost canonical status with such a discrepancy in its background? In our eyes, the explanation may go back to the very different comparison capacities of the human mind vs. the computer, leading to differently robust deductions, and for a remedy to this situation one needs to call in data science. Namely if we want to apply machine learning for motif identification and extraction, we need suitable datasets which enable research teams to replicate each other's results. Below we take a step in this direction.

The structure of this paper is as follows. In Section 2, we bring examples of related research.  In Section 3, Ashliman’s Folktexts tale collection is introduced. In Section 4 we explain our motivation to support reproducible research in computational folkloristics, with Section 5 offering details of data harvesting and cleaning. Section 6 brings details about the new annotated dataset for machine learning, while in Section 7 we add our conclusions and plans for future research.


# Related research {#related_research}

Instead of enumerating important steps in computational folkloristics \cite{10.1145/2209249.2209267} in a chronological manner, for the sake of brevity we present related progress in the context of \italicize{evolving semantics}. In interaction with vector space based methods and predication based semantic indexing \cite{DBLP:journals/jbi/CohenWSDR12}, this paradigm is emerging, e.g., in digital preservation \cite{kontopoulos2016pericles}, \cite{kontopoulos2016deliverable}. 

2.1 Converging trends

In this broader context, one  can observe two major trends whose convergence will be underlying the results of the next decade. The first is focus on the evolutionary aspect of motif and/or tale type distributions, either with regard to certain tale types \cite{dur17330}, cite\{2d77156b39724b4b86eeca60d508f678}, \cite{45bb5e2c15424346b27631d8e1d16c8a} (eg d’Huy and Tehrani, Berezkin), or geographical distribution of globally occurring motifs \cite{article_1}, or both (refs). Strikingly, there is a certain genetically inspired thinking in the background, ultimately going back to the modelling capacities inherent in Dawkins’ meme theory \cite{r.dawkins1976the-selfish-gen}, comparing tale types as motif sequences to 'narrative DNA'\cite{daranyi2012toward}, cite\{ofek_et_al:OASIcs:2013:4150}, cite\{Meder2016AutomaticEA} (Murphy 2014, 2015), or looking at the evolution of narrative/story networks as a quasi-biological process based on the mutation and recombination of narrative elements \cite{45bb5e2c15424346b27631d8e1d16c8a}. Such views must have been informed by certain similarities with bioinformatics in terms of network motif identification (ref), a problem analog with ours. An early example of this idea was pulished in (Darányi and Ábranyi 1986), unfortunately available in Hungarian only, leading e.g. to (Voigt et al 1999).

The second popular trend is that of computing embeddings by multivariate statistics; this means geometrically located content of increasingly condensed word and sentence semantics and beyond (refs), and must be familiar from information retrieval, machine learning, data science or knowledge representation. Many results of computational folkloristics manifest examples of this paradigm \cite{article}, cite\{info11050236}. Related methods fall back on \italicize{distributional semantics}. However, vector spaces are not really suitable to investigate semantic evolution per se, the notion asking for vector fields instead to model the inherent dynamics of content \cie{Wittek_2015}, \cite{darányi2016physical}. Unfortunately no semantic theory is available to explain factors behind language change or conceptual dynamics \cite{daranyi2013demonstrating} in terms of vector fields for the time being.

Significantly, another method of encoding sentence semantics, reliant on \italicize{compositional semantics}, connects to quantum theory (QT) inspired text processing methods, a research direction in artificial intelligence \cite{DBLP:journals/corr/abs-2101-04255}. The first publications looking at the structural study of Greek mythology from a QT perspective were published a while ago \cite{daranyi2013sphynx}, \cite{DBLP:conf/qi/DaranyiW15}, expected to pave the way for other efforts. 

As the computing of results for the above both trends require datasets, we briefly look at their availability next.

2.3 Databases and datasets

D’Huy et al with U. Datasets extracted from databases must exist but are not published. Berezhkin dataset in Russian only. This is a catch-22 situation: A Dutch will never repeat the experiment and a non-Dutch will never be able to do so. The same holds for Russian, Estonian, Hungarian, etc. The closest to a lingua franca, no pun intended, is to default on English. GS survey returns practically nil. Meder survey, Ilyefalvi, all articles rely on ones of own manufacturing, plus neither are in the public domain. Evolving datasets even less so (Karsdorp 2016). One of the exceptions that qualified in every respect, and was graciously donated to the digital humanities (DH) and data science community, is Prof DL Ashliman’s Folktexts (http).

Tangherlini 2016: Big folklore implies pattern discovery at large but the respective datasets are nowhere to be accessed.

Berezkin 2015b states that only the catalog is placed on the web but not the corresponding files. Research potential thereafter is nominal at best.Site was promised to be opened but I wonder. Cca 50000 abstracts, comparable with Meertens, but short of institutional support, in Russian only.

# The Ashliman Folktexts collection {#aft_collection}

(We should decide if AFT refers to the Ashliman Folktexts collection, or Annotated Folktales, but in the title and elsewhere we have sticked with the first option this far)

While some previous studies reference the corpus, these often only include a smaller portion of the entire set of texts \cite{reiter_nlp_2014}.

# Support for Reproducibility in Folklore Studies

Reproducibility is a defining characteristic of science, yet a wide gamut of scientific fields have been plagued by a "replicability crisis": a situation where trusted research findings have been impossible to reproduce [cite].  While the problem has come to the fore in the health and social sciences, it has been acknowledged in disciplines as broad as archaeology [cite], political science [cite], biology [cite], and economics [cite]. 

Strides have been made in the digital humanities to emulate these efforts, with the *Journal of Open Humanities Data*  being a noteworthy exception to the more common practice.

Reproducible research entails that study results be accompanied by:

1. a detailed description of the methods used to obtain and operate on the data
2. the full dataset(s) used in the study 
3. the full code used to transform the data and compute the results

## Guiding Principles

The following features guided our selection of tools and format for the code and data: 

- *Open data*: In order to use tale data consistently, it must be made freely and openly available to anyone. The dataset is therefore distributed under a Creative Commons license [cite].
- *Extensible data*: The dataset can be added to or modified, in order to develop a more complete repository of tales.  This can be done by submitting pull requests to the project's GitHub repository (see Sect. \ref{grow} for additional details).
- *Open code*: Allowing any user to view and run the code that produces the dataset, as well as downstream analyses which use the dataset.  This allows for inspection, refinement and reasoning about the effects of transformation and statistical modeling on the data.
- *Common form*: We have chosen to use the dataframe as the structure of the dataset, and specifically the "tidy" dataframe described by Wickham, in which (a) Each variable forms a column, (b) Each observation forms a row, and (c) a single type of observational unit forms the dataframe \cite{wickham_tidy_2014}.
- *Common tools*: The data must also be structured in a way that allows for use with the standard tools of the trade of data science.  These tools are continuously evolving, yet the dataframe is likely to continue to be common object across R (in `tidyverse`) and Python (in `pandas`).  In addition, it can be read easily from a `.csv` format by Excel users to allow for ease of investigation.
- *Modifiable form*: Text analysis has traditionally used other types of data structures to model its quantitative features (e.g. document-term matrices, term co-occurrence matrices), and dataframes have been incorporated into tidy data workflows and available packages such as `quanteda` or `tidytext`.  This allows for reshaping the data into sparse matrices, nested structures, and graph-based structures as dictated by the needs of a given analysis, while starting from a common source dataset (i.e. the `aft`).

## Growing the Corpus {#grow}

- motifs, tale types and tale corpus are incomplete, but that does not mean they should be thrown out
- need structure for adding new tales
- pull request provides structure for submission and review of changes
- this can also be used to identify and correct errors (so publish and PR)
- for reproducible research, articles using the datasets should use the url with the current commit's SHA to indicate the state of the dataset at the time the analysis was run.


# Data Harvesting and Cleaning {#data_cleaning}

## Steps

```{r site_links}
# See fetch_ashliman.R script for source
site_url <- "http://www.pitt.edu/~dash/folktexts.html"

pg <-
  read_html(site_url) %>%
  html_nodes("a") 

# Obtain urls for all sub-pages on Folktexts website, filtering for annotated ones

links <- 
  tibble(
    type_name = pg %>% html_text(),
    url = pg %>% html_attr("href")
  ) %>%
  filter(!is.na(url)) %>%
  mutate(
    rev_url = case_when(
      str_detect(url,"^http") ~ url,
      T ~ paste0("http://www.pitt.edu/~dash/",url)
    ),
    short_name = str_remove(url,".html")
  ) %>%
  filter(!str_detect(url,"^http")) %>%
  filter(!str_detect(url,"#[a-z]$")) %>% # Only single letter
  filter(!str_detect(url,"^ashliman.html$|^folktexts.html$|^folktexts2.html$|^folklinks.html$")) %>%
  filter(!str_detect(type_name,regex("essay",ignore_case = T))) %>%
  distinct() %>%
  # Recode html names which do not contain their tale types
  mutate(
    rev_name = recode(
      short_name,
      `alibaba`      = "type0676",
      `animalindian` = "type0402",
      `norway034`    = "type0402",
      `norway133`    = "type0133",
      `type2033`     = "type0020c",
      `friday`       = "type0779j*",
      `frog`         = "type0440",
      `hand`         = "type0958e*",
      `type1066`     = "type1343",
      `hog`          = "type0441",
      `monkey`       = "type0441",
      `melusina`     = "type4080",
      `norway010`    = "type1408",
      `norway120`    = "type0313",
      `midwife`      = "type5070"
    )
  ) %>%
  filter(str_detect(rev_name,regex("^type",ignore_case = T))) %>%
  mutate(
    atu_id = str_remove(rev_name,"^type"),
    atu_id = str_remove(atu_id,"jack$|ast$|#longfellow$")
  ) %>%
  select(type_name,atu_id,url = rev_url)
```

Web-scraping of the AFT site was completed using the `rvest` package in the `R` statistical programming language.  The full script is available on GitHub, and the following high-level summary of data-cleaning steps is provided to allow for an understanding of the methods used and their limitations:

1. Obtain URLs and associated label text for all "child" pages of the main website to create a dataframe of page names and URLs.^[The main URL for the site is `http://www.pitt.edu/~dash/folktexts.html`]
2. Remove any links pointed to external websites, since these would require separate web-scraping logic to be developed.
3. Retain all links with the form `type...`, which Ashliman used to denote pages containing tales belonging to a type.  Recode links which do not follow this form, but which contain tales belonging to an ATU type.  For example, the page for *Animal Brides and Animal Bridegrooms* was recoded as belonging to ATU type 0402.
4. Extract the ATU type ID from the URL for each page.

The steps above result in a dataframe listing `r nrow(links)` webpages, each associated with a tale type and containing the page name, the page URL, and the associated ATU ID for each.  This list of page URLs was looped through, using the following steps to the HTML within each page:

5. Extract HTML nodes from the page using CSS selectors (i.e. `body`, `h1`, `li` , `p`, `h3`, `a`) and create a dataframe using the text, name and attribute elements of the nodes.
6. Remove the table of contents and other superfluous text other than the tales, their titles, and other associated metadata (e.g. source documents, notes, etc.).
7. Since not all paragraphs had HTML tags, using a straightforward scraping technique would result in tales with missing sections.  Therefore, we separated the `body` of each page into a separate dataframe, unnested the text by lines,^[Using the `tidytext::unnest_tokens()` function.] and used a fuzzy-joining method to align the missing body text with the well-formatted HTML.^[Using the `fuzzyjoin::stringdist_full_join()` function, we used the *Jaro-Winkler* method and set the maximum distance for a match to 1.]
8. Join to the dataframe of extracted data elements from other URLs.

The resulting dataframe compiled the available tales from the original list of `r nrow(links)` webpages.  To this dataframe, the following steps were applied:

9. Select the longest `text`, choosing between the tagged HTML version and the version extracted from the `body`.
10. Select the available metadata from the tagged HTML versions where those existed, using the alternate versions only if those were `NA`.
11. Remove irrelevant entries using regular expressions.
12. Create unique tale titles where these were duplicated across multiple variants of tales.
13. Clean tale text data (e.g. removing remnant HTML tags, extra spaces, replacing internal double quotes with single quotes).

## Limitations

- Unable to scrape broken links
- Following pages from the initial set of URLs were unable to be scraped, due to errors generated in the session.
- Only one tale type per tale, intent is to store multiple ATUs as a nested list
- The `provenance` field is still messy, since multiple variables (i.e. country, region, tale collection) are still stored in a single column

# Features of the Annotated FolkTales (`aft`) dataset {#data_features}

```{r aft}
aft <- read_csv("../../data/aft.csv")
```

## Data Dictionary

The `aft` (i.e. *Annotated Folk Tales*) dataframe contains `r nrow(aft)` rows, each corresponding to a single tale.  Its `r ncol(aft)` columns are described briefly below:

- `type_name` : The name associated with the Aarne-Thompson-Uther (ATU) tale type identifier.
- `atu_id` : The Aarne-Thompson-Uther (ATU) tale type identifier which classifies the tale.
- `tale_title` : The title of the tale. 
- `provenance` : The person, place or tradition from which the tale came.  In Ashliman's collection, this refers variously to the person recording the tales (*e.g. Giambattista Basile*), the country or region from which the version of the tale came (*e.g. North Africa*), or the larger collection of tales in which the tale is found (*e.g. The Kathasaritsagara*). 
- `notes` : Additional notes related to the tale.
- `source` : The bibliographic citation for the original published source of the tale.
- `copyright` : Any copyright information published alongside the tales in their scraped sources.
- `text` : The full text of the tale identified in `tale_title`.
- `data_source` : The source of the annotated tales.  At the time of this writing, the source of all tales is "Ashliman's Folktexts", but this will change as the [dataset grows](#grow).
- `date_obtained` : The date on which the data set identified as a `data_source` was last downloaded and compiled.

## Descriptive Statistics

#### Length of tales. 

The `r nrow(aft)` tales in the dataset average `r round(mean(str_count(aft$text, '\\w+')), digits = 1)` words in length, though the individual texts vary with a minimum of `r min(str_count(aft$text, '\\w+'))` words and a maximum of `r max(str_count(aft$text, '\\w+'))`.  The histogram below shows the distribution of tale lengths:

```{r word_hist, fig.asp = 0.36}

aft %>%
  mutate(count_words = str_count(aft$text, '\\w+')) %>%
  ggplot(aes(x = count_words)) +
  geom_histogram(
    binwidth = 250
  ) +
  labs(
    title = "Distribution of tale text length",
    caption = "Each bar represents a range of 250 (e.g. 0-250, 251-500, etc.)",
    x = "# of words",
    y = "# of tales"
  ) +
  theme_bar

```

#### Number of tales by ATU type

```{r by_type}

by_type <- 
  aft %>%
  group_by(atu_id) %>%
  summarize(
    type_name = min(type_name),
    n_tales = n()
  )

```

The tales compiled in the `aft` are annotated by Aarne-Thompson-Uther (ATU) tale type, and represent `r n_distinct(aft$atu_id)` distinct types.  There are an average of `r mean(by_type$n_tales)` tales in each tale type, with a range of `r min(by_type$n_tales)` to `r max(by_type$n_tales)`.

```{r type_hist, fig.asp = 0.36}

by_type %>%
  ggplot(aes(x = n_tales)) +
  geom_histogram() +
  labs(
    title = "Distribution of tale type membership",
    subtitle = "How many tales are included in each type?",
    x = "# of tales",
    y = "# of tale types"
  ) +
  theme_bar

```

The tale types with the largest representative group of tales in the corpus is shown below:

```{r, fig.asp = 0.5}

by_type %>%
  top_n(10,n_tales) %>%
  mutate(label = fct_reorder(paste0(type_name," (",atu_id,")"),n_tales)) %>%
  ggplot(aes(x = n_tales, y = label)) +
  geom_point(size = 3) +
  geom_segment(aes(yend = label), xend = 0) +
  labs(
    title = "Top tale types",
    subtitle = "Ten tale types with the largest number of representative tales",
    y = "ATU Tale Type",
    x = "# of tales"
  ) +
  theme_bar_flip +
  xlim(0,25)

```

# Conclusion and Future Research {#conclusion}

## Future 

The intent of the repository is to provide:

- a set of all defined mythological and folktale motifs
- a set of 'types', or recipes describing a sequence of motifs which are commonly used together in myths and tales
- a collection of myth and tale texts that have been annotated as belonging to a 'type'




