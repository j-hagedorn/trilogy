---
title: A bag-of-tales from Santa
subtitle: Converting the Ashliman Folktexts Collection into a dataset for machine learning
titlerunning: Bag-of-tales
authorrunning: Darányi, S. & Hagedorn, J.
thanks: | 
    Grants or other notes about the article that should go on the front 
    page should be placed here. General acknowledgments should be placed at the
    end of the article.

authors: 
- name: Sándor Darányi
  address: Swedish School of Library and Information Science, University of Borås
  email: abc@def
  
- name: Joshua Hagedorn
  address: Department of ZZZ, University of WWW
  email: josh.hagedorn@gmail.com

keywords:
- folktale
- mythology
- motif
- reproducibility
- machine learning

#PACS: 
#- PAC1
#- superPAC
    
# MSC:
# - MSC code 1
# - MSC code 2

abstract: |
  Computational motif identification in folktales is an open research problem. To move ahead in this area, the field would benefit from shared test data for machine learning, putting experimentation in focus. Folklore databases including text collections in multiple languages do exist, but not in dataset form for data science, and are currently not shared, making their results non-reproducible, an obstacle to scientific progress. The need for significant preprocessing adds insult to injury, rendering the outcome both incomparable and subject to multidisciplinary criticism. As a first step to remedy this problem, we report work in progress, having converted the Ashliman Folktexts Collection into a public dataset for supervised tale type learning, itself a precondition for scalable motif identification. In the future, this dataset can be upgraded in several respects to serve as the basis for springboard experiments with the Thompson Motif Index and the Aarne-Thompson-Uther tale typology, paving the way for ontology development.

bibliography: bibliography.bib
biblio-style: spbasic
# bibstyle options spbasic(default), spphys, spmpsci
output: rticles::springer_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = F,message = F,error = F,warning = F,
  fig.width=6,fig.asp = 0.618,dpi=300,dev = "cairo_pdf"
)
library(tidyverse); library(httr); library(rvest); library(tidytext); library(fuzzyjoin)
```

```{r themes, echo=FALSE, message=FALSE}
extrafont::loadfonts(device="win")
font_family = "Gill Sans MT"

theme_bar <- 
  theme_minimal() + 
  theme(
    plot.title.position = "plot",
    text = element_text(family = font_family),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(size=.1, color="grey" ),
    panel.grid.minor.y = element_line(size=.1, color="grey" ),
    axis.text.x.bottom = element_text()
  )

theme_bar_flip <- 
  theme_minimal() + 
  theme(
    plot.title.position = "plot",
    text = element_text(family = font_family),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_line(size=.1, color="grey" ),
    panel.grid.minor.x = element_line(size=.1, color="grey" ),
    axis.text.x.bottom = element_text()
  )
```

# Introduction {#intro}

Ever since the concept of a motif was introduced some 200 years ago, the quest to identify content elements above word level has been a standard preoccupation in literary science (Frenzel 1976, Seigneuret 1988). There a motif stands for a recurrent theme, whereas in musicology, a motive is considered "the smallest structural unit possessing thematic identity" (White 1976). In a similar vein, Stith Thompson defined motifs in folktale research as "the smallest element in a tale having the power to persist in tradition" (1946).

A sufficient overlap between these definitions suggests that such higher order content units exist as narrative building blocks in a generic sense, but their automatic extraction by computational means has eluded folk narrative studies so far (Darányi and Lendvai 2010). In spite of the suggestion that topics identified by Labeled Latent Dirichlet Allocation (L-LDA) had an analogous function with motifs in a database of Dutch and Frisian folktales (Karsdorp and van den Bosch 2013), we consider finding characteristic patterns of semantic content an open research problem. One reason for our skepticism is that in Thompson’s Motif Index of Folk Literature (TMI, 1955-58) alone, over 45000 motifs are listed on a global scale, but many more e.g. regional motif indexes exist whose material would doubtlessly inflate that number. As we will argue below, digital humanities (DH) in general, and folk narrative studies in particular, are not up to the task of scalable pattern hunt yet.

<!-- Add numbers from TMI/ATU mapping -->

Our research problem for the current paper is this: consider the case of two standard reference tools, the TMI, and the Aarne-Thompson-Uther tale type index (ATU, 2004). A count in the TMI indicated the presence of …. motifs, whereas Yarlott and Finlayson counted 46,248 motifs and sub-motifs from over 614 collections, 41,796 of which had references to tales or tale types (2016). However, based on our count the ATU uses only …. (…%) of them to model tale structures as motif strings. One is then prompted to ask, where have …. % of motifs in the TMI disappeared, and how can an important monograph acquire almost canonical status with such a discrepancy in its background? In our eyes, the explanation may go back to the very different comparison capacities of the human mind vs. the computer, leading to differently robust deductions, and for a remedy to this situation one needs to call in data science. Namely if we want to apply machine learning for motif identification and extraction, we need suitable datasets which enable research teams to replicate each other's results. Below we take a step in this direction.

The structure of this paper is as follows. In Section 2, we bring examples of related research.  In Section 3, we discuss Ashliman’s Folktexts tale collection. In Section 4 data cleaning and data restructuring aspects are presented which lead to a dataset for machine learning. Section 5 is a brief outline of potential research directions for the future.


# Relevant Related Research {#related_research}



# The Ashliman Folktexts collection {#aft_collection}

```{r site_links}
# See fetch_ashliman.R script for source
site_url <- "http://www.pitt.edu/~dash/folktexts.html"

pg <-
  read_html(site_url) %>%
  html_nodes("a") 

# Obtain urls for all sub-pages on Folktexts website, filtering for annotated ones

all_links <- 
  tibble(
    type_name = pg %>% html_text(),
    url = pg %>% html_attr("href")
  ) %>%
  filter(!is.na(url)) %>%
  mutate(
    rev_url = case_when(
      str_detect(url,"^http") ~ url,
      T ~ paste0("http://www.pitt.edu/~dash/",url)
    ),
    short_name = str_remove(url,".html")
  ) %>%
  filter(!str_detect(url,"^http")) %>%
  filter(!str_detect(url,"#[a-z]$")) %>% # Only single letter
  filter(!str_detect(url,"^ashliman.html$|^folktexts.html$|^folktexts2.html$|^folklinks.html$")) %>%
  filter(!str_detect(type_name,regex("essay",ignore_case = T))) %>%
  distinct() %>%
  # Recode html names which do not contain their tale types
  mutate(
    rev_name = recode(
      short_name,
      `alibaba`      = "type0676",
      `animalindian` = "type0402",
      `norway034`    = "type0402",
      `norway133`    = "type0133",
      `type2033`     = "type0020c",
      `friday`       = "type0779j*",
      `frog`         = "type0440",
      `hand`         = "type0958e*",
      `type1066`     = "type1343",
      `hog`          = "type0441",
      `monkey`       = "type0441",
      `melusina`     = "type4080",
      `norway010`    = "type1408",
      `norway120`    = "type0313",
      `midwife`      = "type5070"
    )
  ) 

links <-
  all_links %>%
  filter(str_detect(rev_name,regex("^type",ignore_case = T))) %>%
  mutate(
    atu_id = str_remove(rev_name,"^type"),
    atu_id = str_remove(atu_id,"jack$|ast$|#longfellow$")
  ) %>%
  select(type_name,atu_id,url = rev_url)
```

The 'Folktexts' site has been populated and maintained since 1996 by D.L. Ashliman, professor emeritus from the University of Pittsburgh.  While some other sites may have a more lavish design, Ashliman's is the largest and most extensively annotated.  It serves as a respected scholarly resource for folklorists, with a large and curated set of tale texts.  While we have only included tales from pages with clear ATU annotations (n = `r nrow(links)` pages), the total content of the website is much larger (n = `r nrow(all_links)` pages), and includes various creation myths, stories of changelings, Faust legends, and more.  

Despite the richness of this resource, it has not frequently been used in folklore research as a larger corpus.  While some previous studies reference the Ashliman corpus, these often only include a smaller portion of the entire set of texts \cite{reiter_nlp_2014}.  To our knowledge, none of the published studies provide an openly-accessible corpus of the data for use in promoting subsequent research.

# Support for Reproducibility in Folklore Studies

Reproducibility is a defining characteristic of science, yet a wide gamut of scientific fields have been plagued by a "replicability crisis": a situation where trusted research findings have been impossible to reproduce [cite].  While the problem has come to the fore in the health and social sciences, it has been acknowledged in disciplines as broad as archaeology [cite], political science [cite], biology [cite], and economics [cite]. 

Reproducible research entails that study results be accompanied by:

1. a detailed description of the methods used to obtain and operate on the data
2. the full dataset(s) used in the study 
3. the full code used to transform the data and compute the results

In recent years some strides have been made in the digital humanities to emulate these efforts, with the *Journal of Open Humanities Data*  being a noteworthy exception to the more common practice.

## Guiding Principles

The following features guided our selection of tools and format for the code and data: 

- *Open data*: In order to use tale data consistently, it must be made freely and openly available to anyone. The dataset is therefore distributed under a Creative Commons license [cite].
- *Extensible data*: The dataset can be added to or modified, in order to develop a more complete repository of tales.  This can be done by submitting pull requests to the project's GitHub repository (see Sect. \ref{grow} for additional details).
- *Open code*: Allowing any user to view and run the code that produces the dataset, as well as downstream analyses which use the dataset.  This allows for inspection, refinement and reasoning about the effects of transformation and statistical modeling on the data.
- *Common form*: We have chosen to use the dataframe as the structure of the dataset, and specifically the "tidy" dataframe described by Wickham, in which (a) Each variable forms a column, (b) Each observation forms a row, and (c) a single type of observational unit forms the dataframe \cite{wickham_tidy_2014}.
- *Common tools*: The data must also be structured in a way that allows for use with the standard tools of the trade of data science.  These tools are continuously evolving, yet the dataframe is likely to continue to be common object across R (in `tidyverse`) and Python (in `pandas`).  In addition, it can be read easily from a `.csv` format by Excel users to allow for ease of investigation.
- *Modifiable form*: Text analysis has traditionally used other types of data structures to model its quantitative features (e.g. document-term matrices, term co-occurrence matrices), and dataframes have been incorporated into tidy data workflows and available packages such as `quanteda` or `tidytext`.  This allows for reshaping the data into sparse matrices, nested structures, and graph-based structures as dictated by the needs of a given analysis, while starting from a common source dataset (i.e. the `aft`).

## Growing the Corpus {#grow}

- motifs, tale types and tale corpus are incomplete, but that does not mean they should be thrown out
- only in English
- need structure for adding new tales
- pull request provides structure for submission and review of changes
- this can also be used to identify and correct errors (so publish and PR)
- for reproducible research, articles using the datasets should use the url with the current commit's SHA to indicate the state of the dataset at the time the analysis was run.


# Data Harvesting and Cleaning {#data_cleaning}

## Steps



Web-scraping of the AFT site was completed using the `rvest` package in the `R` statistical programming language.  The full script is available on GitHub, and the following high-level summary of data-cleaning steps is provided to allow for an understanding of the methods used and their limitations:

1. Obtain URLs and associated label text for all "child" pages of the main website to create a dataframe of page names and URLs.^[The main URL for the site is `http://www.pitt.edu/~dash/folktexts.html`]
2. Remove any links pointed to external websites, since these would require separate web-scraping logic to be developed.
3. Retain all links with the form `type...`, which Ashliman used to denote pages containing tales belonging to a type.  Recode links which do not follow this form, but which contain tales belonging to an ATU type.  For example, the page for *Animal Brides and Animal Bridegrooms* was recoded as belonging to ATU type 0402.
4. Extract the ATU type ID from the URL for each page.

The steps above result in a dataframe listing `r nrow(links)` webpages, each associated with a tale type and containing the page name, the page URL, and the associated ATU ID for each.  This list of page URLs was looped through, using the following steps to the HTML within each page:

5. Extract HTML nodes from the page using CSS selectors (i.e. `body`, `h1`, `li` , `p`, `h3`, `a`) and create a dataframe using the text, name and attribute elements of the nodes.
6. Remove the table of contents and other superfluous text other than the tales, their titles, and other associated metadata (e.g. source documents, notes, etc.).
7. Since not all paragraphs had HTML tags, using a straightforward scraping technique would result in tales with missing sections.  Therefore, we separated the `body` of each page into a separate dataframe, unnested the text by lines,^[Using the `tidytext::unnest_tokens()` function.] and used a fuzzy-joining method to align the missing body text with the well-formatted HTML.^[Using the `fuzzyjoin::stringdist_full_join()` function, we used the *Jaro-Winkler* method and set the maximum distance for a match to 1.]
8. Join to the dataframe of extracted data elements from other URLs.

The resulting dataframe compiled the available tales from the original list of `r nrow(links)` webpages.  To this dataframe, the following steps were applied:

9. Select the longest `text`, choosing between the tagged HTML version and the version extracted from the `body`.
10. Select the available metadata from the tagged HTML versions where those existed, using the alternate versions only if those were `NA`.
11. Remove irrelevant entries using regular expressions.
12. Create unique tale titles where these were duplicated across multiple variants of tales.
13. Clean tale text data (e.g. removing remnant HTML tags, extra spaces, replacing internal double quotes with single quotes).

## Limitations

- Unable to scrape broken links
- Following pages from the initial set of URLs were unable to be scraped, due to errors generated in the session.
- Only one tale type per tale, intent is to store multiple ATUs as a nested list
- The `provenance` field is still messy, since multiple variables (i.e. country, region, tale collection) are still stored in a single column

# Features of the Annotated FolkTales (`aft`) dataset {#data_features}

```{r aft}
aft <- read_csv("../../data/aft.csv")
```

## Data Dictionary

The `aft` (i.e. *Annotated Folk Tales*) dataframe contains `r nrow(aft)` rows, each corresponding to a single tale.  Its `r ncol(aft)` columns are described briefly below:

- `type_name` : The name associated with the Aarne-Thompson-Uther (ATU) tale type identifier.
- `atu_id` : The Aarne-Thompson-Uther (ATU) tale type identifier which classifies the tale.
- `tale_title` : The title of the tale. 
- `provenance` : The person, place or tradition from which the tale came.  In Ashliman's collection, this refers variously to the person recording the tales (*e.g. Giambattista Basile*), the country or region from which the version of the tale came (*e.g. North Africa*), or the larger collection of tales in which the tale is found (*e.g. The Kathasaritsagara*). 
- `notes` : Additional notes related to the tale.
- `source` : The bibliographic citation for the original published source of the tale.
- `copyright` : Any copyright information published alongside the tales in their scraped sources.
- `text` : The full text of the tale identified in `tale_title`.
- `data_source` : The source of the annotated tales.  At the time of this writing, the source of all tales is "Ashliman's Folktexts", but this will change as the [dataset grows](#grow).
- `date_obtained` : The date on which the data set identified as a `data_source` was last downloaded and compiled.

## Descriptive Statistics

#### Length of tales. 

The `r nrow(aft)` tales in the dataset average `r round(mean(str_count(aft$text, '\\w+')), digits = 1)` words in length, though the individual texts vary with a minimum of `r min(str_count(aft$text, '\\w+'))` words and a maximum of `r max(str_count(aft$text, '\\w+'))`.  The histogram below shows the distribution of tale lengths:

```{r word_hist, fig.asp = 0.36}

aft %>%
  mutate(count_words = str_count(aft$text, '\\w+')) %>%
  ggplot(aes(x = count_words)) +
  geom_histogram(
    binwidth = 250
  ) +
  labs(
    title = "Distribution of tale text length",
    caption = "Each bar represents a range of 250 (e.g. 0-250, 251-500, etc.)",
    x = "# of words",
    y = "# of tales"
  ) +
  theme_bar

```

#### Number of tales by ATU type

```{r by_type}

by_type <- 
  aft %>%
  group_by(atu_id) %>%
  summarize(
    type_name = min(type_name),
    n_tales = n()
  )

```

The tales compiled in the `aft` are annotated by Aarne-Thompson-Uther (ATU) tale type, and represent `r n_distinct(aft$atu_id)` distinct types.  There are an average of `r mean(by_type$n_tales)` tales in each tale type, with a range of `r min(by_type$n_tales)` to `r max(by_type$n_tales)`.

```{r type_hist, fig.asp = 0.36}

by_type %>%
  ggplot(aes(x = n_tales)) +
  geom_histogram() +
  labs(
    title = "Distribution of tale type membership",
    subtitle = "How many tales are included in each type?",
    x = "# of tales",
    y = "# of tale types"
  ) +
  theme_bar

```

The tale types with the largest representative group of tales in the corpus is shown below:

```{r, fig.asp = 0.5}

by_type %>%
  top_n(10,n_tales) %>%
  mutate(label = fct_reorder(paste0(type_name," (",atu_id,")"),n_tales)) %>%
  ggplot(aes(x = n_tales, y = label)) +
  geom_point(size = 3) +
  geom_segment(aes(yend = label), xend = 0) +
  labs(
    title = "Top tale types",
    subtitle = "Ten tale types with the largest number of representative tales",
    y = "ATU Tale Type",
    x = "# of tales"
  ) +
  theme_bar_flip +
  xlim(0,25)

```

# Conclusion and Future Research {#conclusion}

## Future 

The intent of the repository is to provide:

- a set of all defined mythological and folktale motifs
- a set of 'types', or recipes describing a sequence of motifs which are commonly used together in myths and tales
- a collection of myth and tale texts that have been annotated as belonging to a 'type'




