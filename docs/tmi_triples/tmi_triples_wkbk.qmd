---
title: "Motif Triples"
execute: 
  eval: false
  echo: false
  warning: false
  error: false
format: html
editor: visual
---

```{r setup, include=FALSE}
#| eval: true
library(tidyverse)
# knitr::opts_chunk$set(echo = F, warning = F, message = F)
# aft <- read_csv("../../data/aft.csv")
# atu <- read_csv("../../data/atu.csv")
```

## Triple creation

### Import raw text

```{r}
#| eval: true
tmi <- read_csv("../../data/tmi.csv")
tmi_text <- tmi %>% select(id,motif_name) %>% slice(1:35)

head(tmi_text)
```

### Text cleaning and pre-processing

### Text tagging and coreference resolution

Tag text using *Spacy* to obtain: \['Document', 'Sentence', 'Word', 'Word', 'EntityType', 'EntityIOB', 'Lemma', 'POS', 'POSTag', 'Start', 'End', 'Dependency'\]

```{python import_load}

import spacy
import re
import pandas as pd
import textacy
nlp = spacy.load("en_core_web_sm")

tmi_text_py = r.tmi_text
docs = list(nlp.pipe(tmi_text_py.motif_name))
```

### Get POS and token attributes

```{python tokenize}

def extract_tokens_plus_meta(doc:spacy.tokens.doc.Doc):
    """Extract tokens and metadata from individual spaCy doc."""
    return [
        (i.text, i.i, i.lemma_, i.ent_type_, i.ent_iob_,
         i.pos_, i.tag_, i.dep_, i.is_sent_start, i.is_sent_end,
         i.is_alpha, i.is_digit, i.is_punct) for i in doc
    ]

def tidy_tokens(docs):
    """Extract tokens and metadata from list of spaCy docs."""
    
    cols = [
        "doc_id", "token", "token_order", "lemma","ent_type","ent_iob",
        "pos","tag","dep", "is_sent_start","is_sent_end", 
        "is_alpha", "is_digit", "is_punct"
    ]
    
    meta_df = []
    for ix, doc in enumerate(docs):
        meta = extract_tokens_plus_meta(doc)
        meta = pd.DataFrame(meta)
        meta.columns = cols[1:]
        meta = meta.assign(doc_id = ix).loc[:, cols]
        meta_df.append(meta)
        
    return pd.concat(meta_df) 

df_tokens = tidy_tokens(docs)

```

### Get SVO triples

Note that the following script still needs to be optimized to apply to a dataframe so that we can retain additional metadata from the source dataframe.

#### Using pattern matching in spaCy

```{python}
blob_df = tmi_text_py['motif_name'].tolist() 
blob_df = '. '.join(blob_df) 
doc = nlp(blob_df) 
lst_docs = [sent for sent in doc.sents]  

```

```{python}
tmi_text_py['sents'] = tmi_text_py['motif_name'].apply(lambda x: list(nlp(x).sents))
```

#### Using textacy triple extraction

Extract subject-verb-object (SVO) triples from sentences using [function](https://textacy.readthedocs.io/en/latest/api_reference/extract.html#triples) from `textacy` package.

```{python triple_textacy}
dic = {"id":[], "text":[], "entity":[], "relation":[], "object":[]}

for n, sentence in enumerate(lst_docs):
    lst_generators = list(textacy.extract.subject_verb_object_triples(sentence))  
    for sent in lst_generators:
        subj = "_".join(map(str, sent.subject))
        obj  = "_".join(map(str, sent.object))
        relation = "_".join(map(str, sent.verb))
        dic["id"].append(n)
        dic["text"].append(sentence.text)
        dic["entity"].append(subj)
        dic["object"].append(obj)
        dic["relation"].append(relation)

## create dataframe
triple_df = pd.DataFrame(dic)
```

------------------------------------------------------------------------

### Coreference Resolution

Still need work on this, see:

-   [Sample project](https://github.com/explosion/projects/tree/v3/experimental/coref) using [spaCy experimental](https://github.com/explosion/spacy-experimental)

```{python}
from spacy_experimental.coref.coref_component import DEFAULT_COREF_MODEL 
from spacy_experimental.coref.coref_util 
import DEFAULT_CLUSTER_PREFIX  

coref = nlp.add_pipe("experimental_coref") 
# This usually happens under the hood 
processed = coref(doc) 
```

------------------------------------------------------------------------

Test getting spaCy output as dataframe

```{python}
from dframcy import DframCy
nlp = spacy.load('en_core_web_sm')
dframcy = DframCy(nlp)
doc = dframcy.nlp.pipe(tmi_text_py.motif_name)
annotation_dataframe = dframcy.to_dataframe(doc)
```

```{python}
columns=["id", "text", "start", "end", "pos_", "tag_", "dep_", \
"head", "ent_type_", "lemma_", "lower_", "is_punct", "is_quote", "is_digit"]

def get_features(item):
    doc = dframcy.nlp.pipe(tmi_text_py.motif_name)
    annotation_dataframe = dframcy.to_dataframe(doc, columns=columns)
    annotation_dataframe['index'] = item[0]
    return annotation_dataframe

results = []
for item in tqdm(df.iterrows(), total=df.shape[0]):
    results.append(get_features(item))

features = pd.concat(results)
features
```

Use *Spacy* `neuralcoref` for coreference resolution.

### Chunking

### Relation identification

All the verbs that are extracted from the given text are stored in an array, as possible relation candidates or predicates. Further, the words immediately after the verbs are also taken as adposition predicates.

### Triples creation

The entities that are tagged and relations extracted from the previous steps are combined in this step. The text is read again, and the entities tagged as either subject or object or entity are taken as two entities, and the relation extracted in the previous step is used to form a triple based on the position of the 2 entities and the predicate. The predicate position being in the middle of the 2 entities.

A graph is generated from the triples from the previous state to uncover the relationships between entities. The graph is used to also include the triples between different sentences. Centered on the connections between prepositions, more triples are produced, such as 'in,' 'on,' 'at' within the graph between named entities. Eventually, the triplets generated by these are combined to produce a complete list of triples.

### Triples filtering and lemmatization

To improve the efficiency of the triples, we exclude any triple with a stop word. NLTK stop words are included in the stop words, along with commonly used words like figure or number representing the figure.

The filtered triples obtained from the previous step is further filtered in the sense that only the triples that have either their subject or object in the keyword entity list are included in the final triple list. Further, the subject or object is also stemmed to get the root word and are checked with the keyword entity list for equality.

Finally, the triples are lemmatized so that while creating a knowledge graph, both terms such as 'agent' and 'agents' would be viewed as the same entity.

### Final triples

### Knowledge graph representation

```{r}
library(tidygraph); library(igraph)

triple_df <- py$triple_df

nodes <-
  triple_df %>%
  select(node_key = entity) %>%
  bind_rows(
    triple_df %>% select(node_key = object)
  ) %>%
  distinct(node_key)

edges <-
  triple_df %>%
  select(from = entity,to = object,relation)

graph_obj <- tbl_graph(nodes = nodes,edges = edges)

write_graph(graph_obj, file = "ccbhc_reqs.graphml", format = "graphml")
```
