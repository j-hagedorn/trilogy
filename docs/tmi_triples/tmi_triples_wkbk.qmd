---
title: "Motif Triples"
format: html
editor: visual
---

```{r setup, include=FALSE}
library(tidyverse)
# knitr::opts_chunk$set(echo = F, warning = F, message = F)
# aft <- read_csv("../../data/aft.csv")
# atu <- read_csv("../../data/atu.csv")
```

## Triple creation

### Import raw text

```{r}
tmi <- read_csv("../../data/tmi.csv")
tmi_text <- tmi %>% select(id,motif_name) %>% slice(1:35)
```

### Text cleaning and pre-processing

### Text tagging and coreference resolution

Tag text using *Spacy* to obtain: \['Document', 'Sentence', 'Word', 'Word', 'EntityType', 'EntityIOB', 'Lemma', 'POS', 'POSTag', 'Start', 'End', 'Dependency'\]

```{python}
tmi_text_py = r.tmi_text
import pandas as pd
import spacy
nlp = spacy.load("en_core_web_sm")
import en_core_web_sm
nlp = en_core_web_sm.load()
docs = list(nlp.pipe(tmi_text_py.motif_name))
```

```{python}
from dframcy import DframCy
nlp = spacy.load('en_core_web_sm')
dframcy = DframCy(nlp)
doc = dframcy.nlp.pipe(tmi_text_py.motif_name)
annotation_dataframe = dframcy.to_dataframe(doc)
```

```{python}
columns=["id", "text", "start", "end", "pos_", "tag_", "dep_", \
"head", "ent_type_", "lemma_", "lower_", "is_punct", "is_quote", "is_digit"]

def get_features(item):
    doc = dframcy.nlp.pipe(tmi_text_py.motif_name)
    annotation_dataframe = dframcy.to_dataframe(doc, columns=columns)
    annotation_dataframe['index'] = item[0]
    return annotation_dataframe

results = []
for item in tqdm(df.iterrows(), total=df.shape[0]):
    results.append(get_features(item))

features = pd.concat(results)
features
```

Use *Spacy* `neuralcoref` for coreference resolution.

### Chunking

### Relation identification

All the verbs that are extracted from the given text are stored in an array, as possible relation candidates or predicates. Further, the words immediately after the verbs are also taken as adposition predicates.

### Triples creation

The entities that are tagged and relations extracted from the previous steps are combined in this step. The text is read again, and the entities tagged as either subject or object or entity are taken as two entities, and the relation extracted in the previous step is used to form a triple based on the position of the 2 entities and the predicate. The predicate position being in the middle of the 2 entities.

A graph is generated from the triples from the previous state to uncover the relationships between entities. The graph is used to also include the triples between different sentences. Centered on the connections between prepositions, more triples are produced, such as 'in,' 'on,' 'at' within the graph between named entities. Eventually, the triplets generated by these are combined to produce a complete list of triples.

### Triples filtering and lemmatization

To improve the efficiency of the triples, we exclude any triple with a stop word. NLTK stop words are included in the stop words, along with commonly used words like figure or number representing the figure.

The filtered triples obtained from the previous step is further filtered in the sense that only the triples that have either their subject or object in the keyword entity list are included in the final triple list. Further, the subject or object is also stemmed to get the root word and are checked with the keyword entity list for equality.

Finally, the triples are lemmatized so that while creating a knowledge graph, both terms such as 'agent' and 'agents' would be viewed as the same entity.

### Final triples

### Knowledge graph representation
