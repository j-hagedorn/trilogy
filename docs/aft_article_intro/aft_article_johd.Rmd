---
title: "Bearing a bag-of-tales: An open corpus of annotated folktales for reproducible research"
# author-inputfile: "authors_johd.tex" 
abstract: |
  Motifs in folktales and myths have been identified and articulated by scholars, and the computational identification and discovery of such motifs is an area of ongoing research.  Achieving this goal means meeting scientific requirements (that methods be comparable and replicable) and requirements for collaboration (that multi-disciplinary teams can reliably access data).  To support those requirements, access to consistent reference datasets is needed.  Unfortunately, these datasets are not openly available in a format that supports their use in data science. The minimal reference datasets for motif identification include: known motifs (e.g. the Thompson Motif Index), known sequences of motifs occurring in tales (e.g. the Aarne-Thompson-Uther tale typology), and actual tale texts annotated with these motif sequences.  Here we report work in progress toward this goal, having converted the Ashliman Folktexts collection into a public dataset of annotated tale texts.
keywords: "annotated folktales \\and mythology \\and motif \\and reproducible research \\and machine learning \\and version control"
bibliography: ["bibliography.bib"]
csl: apa.csl
link-citations: true
output:
  
  word_document:

  bookdown::pdf_document2:
    fig_caption: true
    keep_tex: false
    latex_engine: xelatex
    template: johd-template.tex
    md_extensions: +footnotes
    citation_package: biblatex
    dev: pdf
---

```{r knitr_init, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, paged.print=TRUE}
## Global options
library(knitr)
#options(max.print="75")
opts_chunk$set(echo=F,cache=F,prompt=F,tidy=T,comment=NA,message=F,warning=F,out.width='100%',dpi=400,dev = "cairo_pdf")
opts_knit$set(width=100)
# Load required libraries
library(tidyverse); library(httr); library(rvest); library(tidytext); library(fuzzyjoin); library(kableExtra)
# Add libraries to cite here:
# rmdtemplates::write_bib(c("tidyverse", "httr", "rvest"))
```

```{r themes, echo=FALSE, message=FALSE}
extrafont::loadfonts(device="win", quiet = T)
font_family = "Gill Sans MT"

theme_bar <- 
  theme_minimal() + 
  theme(
    plot.title.position = "plot",
    text = element_text(family = font_family,size = 13),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(size=.1, color="grey" ),
    panel.grid.minor.y = element_line(size=.1, color="grey" ),
    axis.text.x.bottom = element_text()
  )

theme_bar_flip <- 
  theme_minimal() + 
  theme(
    plot.title.position = "plot",
    text = element_text(family = font_family),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_line(size=.1, color="grey" ),
    panel.grid.minor.x = element_line(size=.1, color="grey" ),
    axis.text.x.bottom = element_text()
  )

```

# Context and motivation {#intro}

Ever since the concept of a motif was introduced some 200 years ago, the quest to identify elements of content above the word level has been a standard preoccupation in literary science [@frenzel_weltlit_1992; @seigneuret_dictionary_1988]. There, a motif stands for a recurrent theme, whereas in musicology a motif is considered "the smallest structural unit possessing thematic identity" [@white_analysis_1976]. In the vein of folktale research, Stith Thompson defined motifs as "the smallest element in a tale having a power to persist in tradition" [@thompson_folktale_1977].

The sufficient overlap between these definitions suggests that such higher-order content units exist as narrative building blocks in a generic sense, yet their automatic extraction by computational means has eluded folk narrative studies so far [@daranyi_proceedings_2010]. Despite the suggestion that topics identified by Labeled Latent Dirichlet Allocation (L-LDA) had an analogous function with motifs in a database of Dutch and Frisian folktales [@karsdorp_topic_2013], we consider finding characteristic patterns of semantic content an open research problem. One reason for our skepticism is that in Thompson’s Motif Index of Folk Literature [@thompson_motif_index] alone, over 45000 motifs are listed on a global scale, but many more e.g. regional motif indexes exist whose material would doubtlessly inflate that number. As we will argue below, digital humanities (DH) in general, and folk narrative studies in particular, are not up to the task of a scalable pattern hunt yet.  If we want to apply machine learning for motif identification and extraction, we need suitable datasets which enable research teams to replicate each other's results. Below we report an initial step in this direction.

The structure of this paper is as follows. In Section 2, we bring examples of related research.  In Section 3, Ashliman’s Folktexts tale collection is introduced. In Section 4 we explain our motivation to support reproducible research in computational folkloristics, with Section 5 offering details of data harvesting and cleaning. Section 6 brings details about the new annotated dataset for machine learning, while in Section 7 we add our conclusions and plans for future research.

## Related research {#related_research}

As our pilot was not concerned with the structural analysis of folk narratives, we left out from this brief overview significant research results concerning e.g. the automatic detection of Proppian functions [@finlayson_inferring_2016], or their use in ontology building [@declerck_ontologies_2017]. Instead, our focus will be on precursory efforts to automatic motif detection using two standard tools, the Thompson Motif Index (TMI) [@thompson_motif_index], and the Aarne-Thompson-Uther tale typology (ATU) [@uther_types_2004]. Important extensions to these, and to our current work, exist e.g. by [@declerck_porting_2017] and [@declerck_linked_2017].  Also, there exists a thin line between motifs in tales vs. motifs in myths: Walter Burkert proposed that "Myth belongs to the more general class of traditional tale" [@burkert_structure_1982], whereas Martin P. Nilsson argued for the abundance of folktale motifs in Minoan-Mycenaean religion and its successive layers in classical Greek religion [@nilsson_history_1925; @nilsson_mycenaean_1972].

### Converging trends

In a broader context, one can observe two major trends in computational folkloristics [@abello_computational_2012] whose convergence will be underlying the results of the next decade. The first is focus on the evolutionary aspect of motif and/or tale type distributions, either with regard to certain tale types [@silva_comparative_2016; @karsdorp_topic_2013; @karsdorp_retelling_2016; @tehrani_riding_2013; @bortolini_inferring_2017; @dhuy_unbiased_2017] or to the geographical distribution of globally occurring narrative motifs [@thuillard_large-scale_2018]. Strikingly, there is a certain genetically-inspired thinking in the background, perhaps going back to the modeling capacities inherent in Dawkins' meme theory [@dawkins_selfish], comparing tale types as motif sequences to 'narrative DNA' [@daranyi_toward_2012; @ofek_linking_2013; @meder_automatic_2016; @murphy_genotype_2015], or looking at the evolution of narrative/story networks as a quasi-biological process based on the mutation and recombination of narrative elements [@karsdorp_retelling_2016]. Such views possibly rely on certain similarities with bioinformatics in terms of network motif identification [@qin_network_2012], a problem analog with ours. The aforementioned context is that of *evolving semantics*, an emerging research area, e.g. in digital preservation [@kontopoulos2016pericles; @kontopoulos2016deliverable].

The second trend is to use probabilistic and/or multivariate statistical methods for the analysis of binary or non-binary matrices of events over cases, where events can be e.g. index terms, motifs, motif sequences etc., and cases as an umbrella term stand for documents in general, e.g. abstracts describing narratives [@berezkin_spread_2015], tale types [@uther_types_2004], and so on, ultimately constituting text corpora or databases. On such collections, one can then experiment with e.g. sub-corpus topic modeling (STM) by Latent Dirichlet Allocation (LDA) as a means of supervised passage exploration in partly unknown corpora [@tangherlini_trawling_2013].

The little one can say about the plethora of methods tested is that, regardless of the corpora, their regionality and the analytical units whose distributions characterize the body of texts in question, they express similarity between items in terms of distance, with more similar items forming dense groups as the outcome of mass comparison. Cluster analysis [@thuillard_large-scale_2018],  principal component analysis (PCA) [@berezkin_spread_2015], LDA [@karsdorp_topic_2013], deep learning by recurrent neural networks (RNN) [@lo_exploring_2020], or support vector machines (SVM) [@nguyen_automatic_2012] share the same nature of being static snapshots of collections though. Of course there is an inherent contradiction in addressing text evolution, a dynamic phenomenon, by tools tailored to static measurements, but it seems to be the case that vector spaces are not really suitable to investigate semantic evolution per se, the notion asking for vector fields instead [@wittek_monitoring_2015; @daranyi_physical_2016]. Unfortunately, no semantic theory is available to explain factors behind language change or conceptual dynamics [@daranyi_demonstrating_2013] in terms of vector fields for the time being.

Whereas the above approaches, and their extensions to embeddings with increasingly condensed and geometrically located types of meaning [@mikolov2013efficient; @pennington-etal-2014-glove; @rothe-schutze-2015-autoextend; @DBLP:journals/corr/LeM14; @reimers2019sentencebert; @NEURIPS2019_cb12d7f9] rely on *distributional semantics* captured by term co-occurrences, we note in passing that another method of encoding sentence semantics, reliant on *compositional semantics*, connects to quantum theory (QT) inspired text processing methods, a research direction in artificial intelligence [@DBLP:journals/corr/abs-2101-04255]. The first publications looking at the structural study of Greek mythology from a QT perspective were published a while ago [@daranyi_sphynxs_2014; @daranyi_conceptual_2016], expected to pave the way for similar efforts.

As the computing of results for the above both trends require datasets, we briefly look at their availability next.

### Databases and datasets

Progress in computational folkloristics presupposes that experiments can be repeated by international teams to make sure that the results are robust, so that recommended methodologies can emerge based on their ranking. In order to be trusted, results must be replicable, which entails access to public datasets accessible over the Internet with as few bottlenecks as possible. Once teams of folklorists and data scientists share their respective expertise to interpret and curate the results, it will become pointless to lament about any loss of authority, or unsolvable problems in the face of scalability. It’s a tea for two situation.

Moving away now from the problem of text migration combined with text evolution over millennia across the globe, below we focus on a more tractable problem. Having recently scanned the field for open access datasets of ATU-annotated tales in English as a kind of *lingua franca* (no pun intended), i.e. suitable for motif detection by machine learning, we can confirm the following:

- We could not identify such datasets on GitHub^[https://github.com/awesomedata/awesome-public-datasets], Kaggle^[https://www.kaggle.com/datasets] or Google^[https://datasetsearch.research.google.com/];
- Big folklore collections anticipated by @tangherlini_trawling_2013 and @tangherlini_bigfolk are still missing. Based on @meder_dutch_2010 and @ilyefalvi_theoretical_2018, the largest databases seem to be the Dutch Folktale Database of the Meertens Institute, and the Danish Folklore Archive’s Tang Kristensen Collection, the former in the magnitude of around 50.000 texts, the latter at around 35.000 texts [@tangherlini_trawling_2013]. Other important databases exist,  [@berezkin_peopling_2017] but are either beyond public access, or in their original languages only, or both. The notable exception is the Meertens Institute whose texts are of course in Dutch and Frisian plus a number of local dialects, but can be read in English translation as well. Typically, authors who work with these databases extract their research datasets but do not make them public for any number of reasons [@karsdorp_topic_2013; @karsdorp_retelling_2016].
- Other researchers who have shared their data as supporting material for their articles include e.g. [@tehrani_oral_2016; @silva_comparative_2016; @tehrani_riding_2013; @bortolini_inferring_2017]. Also, @declerck_linked_2017 and @declerck_ontologies_2017 report that recently, a large amount of ATU data have been made available online by the Multilingual Folk Tale Database (MFTD)^[https://www.mftd.org], offering also annotation facilities for tales in multilingual versions. While this is a step in the right direction, the data are made available in a format which allows for browsing of portions of the database, but not for easy access to the corpus in its entirety.
- We found only a single recent study [@lo_exploring_2020] which published a corresponding tale corpus to promote reproducibility.^[https://github.com/GossaLo/afr-neural-folktales/]

Among the annotated tale collections which were publicly-available on the internet, the most promising candidate was Prof D.L. Ashliman’s Folktexts collection.  The process of the conversion of this collection to the desired format will be described below. Importantly, Prof. Ashliman graciously agreed to donate his collection to the digital humanities and data science research communities.

### The Ashliman Folktexts collection {#aft_collection}

```{r site_links, eval=FALSE, include=FALSE}
# See fetch_ashliman.R script for source
site_url <- "http://www.pitt.edu/~dash/folktexts.html"
site_url2 <- "http://www.pitt.edu/~dash/folktexts2.html"

pg <-
  read_html(site_url) %>%
  html_nodes("a") 

pg2 <-
  read_html(site_url2) %>%
  html_nodes("a") 

# Obtain urls for all sub-pages on Folktexts website, filtering for annotated ones

all_links <- 
  tibble(
    type_name = pg %>% html_text(),
    url = pg %>% html_attr("href")
  ) %>%
  bind_rows(
    tibble(
      type_name = pg2 %>% html_text(),
      url = pg2 %>% html_attr("href")
    )
  ) %>%
  filter(!is.na(url)) %>%
  mutate(
    rev_url = case_when(
      str_detect(url,"^http") ~ url,
      T ~ paste0("http://www.pitt.edu/~dash/",url)
    ),
    short_name = str_remove(url,".html")
  ) %>%
  filter(!str_detect(url,"^http")) %>%
  filter(!str_detect(url,"#[a-z]$")) %>% # Only single letter
  filter(!str_detect(url,"^ashliman.html$|^folktexts.html$|^folktexts2.html$|^folklinks.html$")) %>%
  filter(!str_detect(type_name,regex("essay",ignore_case = T))) %>%
  distinct() %>%
  # Recode html names which do not contain their tale types
  mutate(
    rev_name = recode(
      short_name,
      `alibaba`      = "type0676",
      `animalindian` = "type0402",
      `norway034`    = "type0402",
      `norway133`    = "type0133",
      `type2033`     = "type0020c",
      `friday`       = "type0779j*",
      `frog`         = "type0440",
      `hand`         = "type0958e*",
      `type1066`     = "type1343",
      `hog`          = "type0441",
      `monkey`       = "type0441",
      `melusina`     = "type4080",
      `norway010`    = "type1408",
      `norway120`    = "type0313",
      `midwife`      = "type5070"
    )
  )

links <-
  all_links %>%
  filter(str_detect(rev_name,regex("^type",ignore_case = T))) %>%
  mutate(
    atu_id = str_remove(rev_name,"^type"),
    atu_id = str_remove(atu_id,"jack$|ast$|#longfellow$|#nigeria$|Blit$|fairy$|#shewolf$|abc$")
  ) %>%
  select(type_name,atu_id,url = rev_url)

saveRDS(all_links,"all_links.rds")
saveRDS(links,"links.rds")

```

```{r}
all_links <- readRDS("all_links.rds")
links <- readRDS("links.rds")
```


The '*Folktexts*' site has been populated and maintained since 1996 by D.L. Ashliman, Professor Emeritus from the University of Pittsburgh.  While some other sites may sport a more lavish design, Ashliman's is the largest and most extensively annotated.  It serves as a respected scholarly resource for folklorists, with a large and curated set of tale texts.  While our dataset includes only tales from pages with clear ATU annotations (n = `r nrow(links)` pages), the total content of the website is much larger (n = `r nrow(all_links)` pages), and includes various creation myths, stories of changelings, Faust legends, and more.  It is the ATU annotation that makes this corpus particularly valuable as a training dataset for classification methods.

Despite the richness of this resource, it has not frequently been used in folklore research as a larger corpus.  Some previous studies reference the Ashliman corpus, yet these often only include a smaller portion of the entire set of texts [@reiter_nlp_2014].  To our knowledge, none of the published studies provide an openly-accessible corpus of the data for use in promoting subsequent research.

# Method

## Support for Reproducibility in Folklore Studies

Reproducibility is a defining characteristic of science, yet a wide gamut of scientific fields have been plagued by a "replicability crisis": a situation where trusted research findings have been impossible to reproduce [@goodman_what_2016; @pasquier_if_2017].  While the problem has come to the fore in the health and social sciences, it has been acknowledged in disciplines as broad as archaeology [@marwick_computational_2017], public health [@harris_use_2018], biology [@kuhne_improving_2009], and economics [@mccullough_open_2009]. 

Reproducible research entails that study results be accompanied by:

1. a detailed description of the methods used to obtain and operate on the data;
2. the full dataset(s) used in the study; 
3. the full code used to transform the data and compute the results.

In recent years the digital humanities have made some strides to emulate these efforts, with venues such as the *Journal of Open Humanities Data*  being a noteworthy exception to the more common practice.

### Guiding Principles

The following features guided our selection of tools and format for the code and data:

- *Open data*: In order to use tale data consistently, it must be made freely and openly available to anyone. The dataset is therefore distributed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)^[https://creativecommons.org/licenses/by-sa/4.0/].
- *Extensible data*: The dataset can be added to or modified, in order to develop a more complete repository of tales.  This can be done by submitting pull requests to the project's GitHub repository.
- *Open code*: Allowing any user to view and run the code that produces the dataset, as well as downstream analyses which use the dataset.  This allows for inspection, refinement and reasoning about the effects of transformation and statistical modeling on the data.
- *Common form*: We have chosen to use the dataframe as the structure of the dataset, and specifically the "tidy" dataframe described by Wickham, in which (a) each variable forms a column, (b) each observation forms a row, and (c) a single type of observational unit forms the dataframe [@wickham_tidy_2014].
- *Common tools*: The data must also be structured in a way that allows for use with the standard tools of the trade of data science.  These tools are continuously evolving, yet the dataframe is likely to continue to be common object across R (in `tidyverse`) and Python (in `pandas`).  In addition, it can be read easily from a `.csv` format by Excel users to allow for ease of investigation.
- *Modifiable form*: Text analysis has traditionally used other types of data structures to model its quantitative features (e.g. document-term matrices, term co-occurrence matrices), and dataframes have been incorporated into tidy data workflows and available packages such as `quanteda` or `tidytext`.  This allows for reshaping the data into sparse matrices, nested structures, and graph-based structures as dictated by the needs of a given analysis, while starting from a common source dataset (i.e. the `aft`).

### Accessing and Growing the Corpus

The current version of the `aft` dataset is located on the `trilogy` GitHub repository, where a vignette^[https://github.com/j-hagedorn/trilogy/blob/master/docs/vignettes/getting_started.md] provides information on how to access the dataset, use it in reproducible research, and improve it.

Existing lists of motifs and tale types are incomplete [@dhuy_unbiased_2017], and thus any corpus of texts is also bound to be incomplete if its contents are limited to tales annotated using these lists.  Open-source *Git* functionality allows motifs, tale types and annotated tales to be added over time, and for the corpus to serve as a communal resource.  We welcome inquiries and suggestions about how best to manage this resource as a "commons" [@vollan_cooperation_2010].

## Data Harvesting and Cleaning {#data_cleaning}

### Steps

Web-scraping of the *Folktexts* site was completed using the `rvest` package in the `R` statistical programming language.  The full script is available on GitHub, and the following high-level summary of data-cleaning steps is provided to allow for an understanding of the methods used and their limitations:

1. Obtain URLs and associated label text for all "child" pages of the main website to create a dataframe of page names and URLs.^[The main URL for the site is `http://www.pitt.edu/~dash/folktexts.html`]
2. Remove any links pointed to external websites, since these would require separate web-scraping logic to be developed.
3. Retain all links with the form `type...`, which Ashliman used to denote pages containing tales belonging to a type.  Recode links which do not follow this form, but which contain tales belonging to an ATU type.  For example, the page for *Animal Brides and Animal Bridegrooms* was recoded as belonging to ATU type 0402.
4. Extract the ATU type ID from the URL for each page.

The steps above result in a dataframe listing `r nrow(links)` webpages, each associated with a tale type and containing the page name, the page URL, and the associated ATU ID for each.  This list of page URLs was looped through, using the following steps to the HTML within each page:

5. Extract HTML nodes from the page using CSS selectors (i.e. `body`, `h1`, `li` , `p`, `h3`, `a`) and create a dataframe using the text, name and attribute elements of the nodes.
6. Remove the table of contents and other superfluous text other than the tales, their titles, and other associated metadata (e.g. source documents, notes, etc.).
7. Since not all paragraphs had HTML tags, using a straightforward scraping technique would result in tales with missing sections.  Therefore, we separated the `body` of each page into a separate dataframe, unnested the text by lines,^[Using the `tidytext::unnest_tokens()` function.] and used a fuzzy-joining method to align the missing body text with the well-formatted HTML.^[Using the `fuzzyjoin::stringdist_full_join()` function, we used the *Jaro-Winkler* method and set the maximum distance for a match to 1.]
8. Join to the dataframe of extracted data elements from other URLs.

The resulting dataframe compiled the available tales from the original list of `r nrow(links)` webpages.  To this dataframe, the following steps were applied:

9. Select the longest `text`, choosing between the tagged HTML version and the version extracted from the `body`.
10. Select the available metadata from the tagged HTML versions where those existed, using the alternate versions only if those were `NA`.
11. Remove irrelevant entries using regular expressions.
12. Create unique tale titles where these were duplicated across multiple variants of tales.
13. Clean tale text data (e.g. removing remnant HTML tags, extra spaces, replacing internal double quotes with single quotes).

### Limitations

Web-scraping is an inherently messy exercise, as the data contained in web pages are often not formatted with the intent of being analyzed.  Due to a broken link in the website, we were unable to obtain tales related to *`r links$type_name[23]`* (`r links$atu_id[23]`).  In addition, the pages for the following tale types were unable to be scraped, due to errors generated in the `R` session: *`r paste(links$type_name[c(70,74,165,172,174,198)],collapse = ", ")`*.

The `provenance` field does not meet the definition of "tidy" outlined above, since multiple types of descriptors (i.e. *country*, *region*, *tale collection*) are stored in a single column.  While additional cleaning may be able to distinguish some of these, we have chosen to leave it as entered in the original.

The final limitation is purposefully adopted for the sake of downstream analyses.  We have included only tales which were annotated with a single tale type, despite the existence of some tales which can be characterized by multiple types.  This decision was made in order to allow for the initial version of the dataset to be simple in its structure, and in order for machine learning to have a relatively unambiguous corpus of motif sequences to match, if using the tales as a training dataset.  If required by future analyses, our intent is to store multiple ATUs as nested lists per tale within the dataframe.

# Results and discussion

## Features of the Annotated Folktales (`aft`) dataset {#data_features}

```{r aft}
aft <- read_csv("../../data/aft.csv")
```

### Data Dictionary

The `aft` (i.e. *Annotated Folktales*) dataframe contains `r nrow(aft)` rows, each corresponding to a single tale.  Its `r ncol(aft)` columns are described briefly below:

- `type_name` : The name associated with the Aarne-Thompson-Uther (ATU) tale type identifier.^[Note that this field currently uses the name of the associated Folktexts webpage, which may not precisely match the ATU description.]
- `atu_id` : The Aarne-Thompson-Uther (ATU) tale type identifier which classifies the tale.
- `tale_title` : The title of the tale. 
- `provenance` : The person, place or tradition from which the tale came.  In Ashliman's collection, this refers variously to the person recording the tales (e.g. Giambattista Basile), the country or region from which the version of the tale came (e.g. North Africa), or the larger collection of tales in which the tale is found (e.g. The Kathasaritsagara). 
- `notes` : Additional notes related to the tale.
- `source` : The bibliographic citation for the original published source of the tale.
- `text` : The full text of the tale identified in `tale_title`.
- `data_source` : The source of the annotated tales.  At the time of this writing, the source of all tales is "Ashliman's Folktexts", but this will change as the dataset grows.
- `date_obtained` : The date on which the data set identified as a `data_source` was last downloaded and compiled.

The table below shows prints the initial characters of fields from the first 6 rows of the dataset, in order to illustrate its appearance:

```{r fig.width= 8}
library(flextable)

table <-
  aft %>%
  select(-notes,-data_source,-date_obtained) %>%
  mutate_all(~str_trunc(.,width = 25)) %>%
  head() 

set_flextable_defaults(
  font.family = font_family,
  font.size = 10,
  font.color = "black",
  # 
  table.layout = "fixed",
  # digits = 1,
  # theme_fun = "theme_box"
)

table %>%
  flextable() %>%
  set_caption("Example output of the dataset")

aft_summary <-
  aft %>%
  # mutate(text = iconv(text, to = 'latin1')) %>%
  mutate(
    n_words = str_count(aft$text, '\\w+'),
    n_sentences = quanteda::nsentence(text),
    n_tokens = quanteda::ntoken(text),
    n_types = quanteda::ntype(text),
    tokens_per_sentence = n_tokens/n_sentences
  ) %>%
  left_join(
    atu %>% select(atu_id,chapter,division,sub_division,tale_name), 
    by = ("atu_id")
  ) %>%
  select(
    atu_id,chapter,division,tale_name,tale_title,
    starts_with("n_"),tokens_per_sentence
  ) 

aft_stats <-
  aft_summary %>%
  summarize(
    n_tales = n(),
    n_types = n_distinct(atu_id),
    avg_tokens = mean(n_tokens),
    avg_sentences = mean(n_sentences)
  )
  

```


### Descriptive Statistics

#### Length of tales. 

The `r nrow(aft)` tales in the dataset average `r round(mean(str_count(aft$text, '\\w+')), digits = 1)` words in length, though the individual texts vary with a minimum of `r min(str_count(aft$text, '\\w+'))` words and a maximum of `r max(str_count(aft$text, '\\w+'))`.  The histogram below shows the distribution of tale lengths:

```{r wordhist, fig.cap="Distribution of tale lengths", fig.asp = 0.36}

aft_summary %>%
  ggplot(aes(x = n_words)) +
  geom_histogram(binwidth = 250) +
  labs(
    caption = "Each bar represents a range of 250 (e.g. 0-250 words, 251-500, etc.)",
    x = "Number of words in tale",
    y = "Number of tales"
  ) +
  theme_bar

ggsave("wordhist.png", units="in", width=5, height=2, dpi=450)

```

#### Number of tales by ATU type

```{r by_type}

by_type <- 
  aft %>%
  group_by(atu_id) %>%
  summarize(
    type_name = min(type_name),
    n_tales = n()
  )

```

The tales compiled in the `aft` data are annotated by Aarne-Thompson-Uther (ATU) tale type, and represent `r n_distinct(aft$atu_id)` distinct types.  There are an average of `r round(mean(by_type$n_tales), digits = 1)` tales in each tale type, with a range of `r min(by_type$n_tales)` to `r max(by_type$n_tales)`.  The tale types with the largest representative group of tales in the corpus is shown below:

```{r, fig.cap="Ten tale types with the largest number of representative tales", fig.asp = 0.5}

by_type %>%
  top_n(10,n_tales) %>%
  mutate(
    label = fct_reorder(
      paste0(
        if_else(is.na(word(type_name,-5,-1)),type_name,word(type_name,-5,-1)),
        " (",atu_id,")"),n_tales
    )
  ) %>%
  ggplot(aes(x = n_tales, y = label)) +
  geom_point(size = 3) +
  geom_segment(aes(yend = label), xend = 0) +
  labs(
    y = "Tale Type (ATU)",
    x = "Number of tales"
  ) +
  theme_bar_flip +
  theme(axis.text=element_text(size=11)) +
  xlim(0,31)

ggsave("lollipop.png", units="in", width=5, height=3, dpi=450)

```

# Implications/Applications {#conclusion}

Under a Creative Commons license, we published on GitHub an open-access, ATU annotated dataset of `r nrow(aft)` tales for motif detection by machine learning. This dataset resulted from the conversion of the Ashliman Folktexts collection, and is hoped to become the core of an expanding assemblage to the same end. Over time it will be updated with additional information to help reproducible experimentation with supervised tale type learning.

In our upcoming work, we plan to extend the repository to provide:

-	a set of all defined mythological and folktale motifs;
-	a set of `types', or recipes describing a sequence of motifs which are commonly used together in myths and tales;
-	a collection of myth and tale texts that have been annotated as belonging to a `type'.”

# Acknowledgements {-}

The authors thank Professor D.L. Ashliman (University of Pittsburgh) for his permission to use the data from his annotated Folktexts as a publicly available resource.

# Competing interests {-}

The authors have no competing interests to declare.

# Supplementary Files

The repository contains our scripts, datasets, and documentation referenced in this article. DOI: https://doi.org/10.5281/zenodo.5835162

# References
